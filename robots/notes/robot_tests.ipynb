{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbdfe9b5-c03b-4373-b3e6-0e4bd75f07f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.16, Python 3.7.11)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import copy \n",
    "import torch \n",
    "import random\n",
    "import pygame\n",
    "import numpy as np\n",
    "import torch.nn as nn \n",
    "import numpy.random as npr\n",
    "from datetime import datetime\n",
    "import torch.nn.functional as F\n",
    "from os.path import join, expanduser\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from utility import *\n",
    "import matplotlib.pyplot as plt \n",
    "from algorithms.networks.tranforms import *\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ede4af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_aug_data = True  # load augmented data from disk?\n",
    "save_aug_data = False  # save loaded files?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1ec60a",
   "metadata": {},
   "source": [
    "### WARNING!!!\n",
    "\n",
    "Loading this from disk takes 6m, 23.1 secs on avhg. Please load the saved file instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f347143",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = join('..', 'experiments', 'inverted_pendulum', 'data_files')\n",
    "\n",
    "if not load_aug_data:\n",
    "    data_aug = DataAugmentor(data_dir=data_dir)\n",
    "    observations, states = data_aug.get_augmented_tensor(rot_step=5)\n",
    "else:\n",
    "    loader = torch.load(join(data_dir, 'obs_state.pth'))\n",
    "    observations, states = loader['observations'], loader['states']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cb7bc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_aug_data:\n",
    "    # save the augmented observations and states to disk\n",
    "    # for faster future loading\n",
    "    data_save = {'observations': observations,\n",
    "    'states': states}\n",
    "    torch.save(data_save, join(data_dir, 'obs_state.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d2feaff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data variance: tensor(0.0467)\n"
     ]
    }
   ],
   "source": [
    "tr_ratio = int(.7*len(observations))\n",
    "Xtr, Xte = observations[:tr_ratio], observations[tr_ratio:]\n",
    "\n",
    "args = Bundle(dict(\n",
    "                seed=123, NSamples=5000,  gpu=0, j=2, gamma=0.7,\n",
    "                batch_size=10, test_batch_size=2,  lr=1e-3,\n",
    "                weight_decay=1e-4,  momentum=.9, \n",
    "                num_epochs=50, best_loss=float(\"inf\"), NRounds=200,\n",
    "                loop_len=100, resume=False, use_lbfgs=False,\n",
    "                log_interval=10, use_cuda=torch.cuda.is_available(),\n",
    "            ))\n",
    "\n",
    "dev = torch.device(\"cuda\" if args.use_cuda else \"cpu\")\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "train_kwargs = {'batch_size': args.batch_size,\n",
    "                    'shuffle': True, 'num_workers': args.j}\n",
    "test_kwargs = {'batch_size': args.test_batch_size,\n",
    "                'shuffle': True, 'num_workers':  args.j}\n",
    "\n",
    "data_kwargs = {'batch_size': args.batch_size,\n",
    "                'shuffle': True, 'num_workers': 2}\n",
    "\n",
    "if args.use_cuda:\n",
    "    cuda_kwargs = {'num_workers': args.j,\n",
    "                    'pin_memory': True,\n",
    "                    'shuffle': True}\n",
    "    data_kwargs.update(cuda_kwargs)\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "train_dataset = RobotDataSingle(Xtr, args.seed, dev)\n",
    "train_loader  = torch.utils.data.DataLoader(train_dataset, **test_kwargs)\n",
    "\n",
    "test_dataset  = RobotDataSingle(Xte, args.seed, device=dev)\n",
    "test_loader   = torch.utils.data.DataLoader(test_dataset, **test_kwargs)\n",
    "\n",
    "# checkpoints_dir = 'data/{}/checkpoints_{}'.format(expt, pol_net.name)\n",
    "# summaries_dir = 'data/{}/summaries_{}'.format(expt, pol_net.name)\n",
    "# \n",
    "# if not os.path.exists(checkpoints_dir):\n",
    "#     os.makedirs(checkpoints_dir)\n",
    "# if not os.path.exists(summaries_dir):\n",
    "#     os.makedirs(summaries_dir)\n",
    "\n",
    "def cast_and_normalise_images(images):\n",
    "        \"\"\"Convert images to floating point with the range [-0.5, 0.5]\"\"\"\n",
    "        return (images.astype(np.float32) / 255.0) - 0.5\n",
    "\n",
    "train_data_variance = torch.var(Xtr / 255.0)\n",
    "print('train data variance: %s' % train_data_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074a50a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see: https://github.com/deepmind/sonnet/blob/v2/examples/vqvae_example.ipynb4\n",
    " \n",
    "class ResidualStack(nn.Module):\n",
    "  def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
    "    super(ResidualStack, self).__init__()\n",
    "    self._num_hiddens = num_hiddens\n",
    "    self._num_residual_layers = num_residual_layers\n",
    "    self._num_residual_hiddens = num_residual_hiddens\n",
    "\n",
    "    self._layers = []\n",
    "    for i in range(num_residual_layers):\n",
    "      conv3 = nn.Conv2D(\n",
    "          out_channels=num_residual_hiddens,\n",
    "          kernel_size=(3, 3),\n",
    "          stride=(1, 1))\n",
    "      conv1 = nn.Conv2D(\n",
    "          out_channels=num_hiddens,\n",
    "          kernel_size=(1, 1),\n",
    "          stride=(1, 1))\n",
    "      self._layers.append((conv3, conv1))\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    h = inputs\n",
    "    activation = nn.ReLU()\n",
    "    for conv3, conv1 in self._layers:\n",
    "      conv3_out = conv3(activation(h))\n",
    "      conv1_out = conv1(activation(conv3_out))\n",
    "      h += conv1_out\n",
    "    return activation(h)  # Resnet V1 style\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "  def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
    "    super(Encoder, self).__init__()\n",
    "    self._num_hiddens = num_hiddens\n",
    "    self._num_residual_layers = num_residual_layers\n",
    "    self._num_residual_hiddens = num_residual_hiddens\n",
    "\n",
    "    self._enc_1 = nn.Conv2D(\n",
    "        in_channels=self._num_hiddens // 2, \n",
    "        out_channels=self._num_hiddens // 2,\n",
    "        kernel_size=(4, 4),\n",
    "        stride=(2, 2))\n",
    "    self._enc_2 = nn.Conv2D(\n",
    "        in_channels=self._num_hiddens,\n",
    "        out_channels=self._num_hiddens,\n",
    "        kernel_size=(4, 4),\n",
    "        stride=(2, 2))\n",
    "    self._enc_3 = nn.Conv2D(\n",
    "        in_channels=self._num_hiddens,\n",
    "        out_channels=self._num_hiddens,\n",
    "        kernel_size=(3, 3),\n",
    "        stride=(1, 1),)\n",
    "    self._residual_stack = ResidualStack(\n",
    "        self._num_hiddens,\n",
    "        self._num_residual_layers,\n",
    "        self._num_residual_hiddens)\n",
    "\n",
    "  def forward(self, x):\n",
    "    activation = nn.ReLU()\n",
    "\n",
    "    h = activation(self._enc_1(x))\n",
    "    h = activation(self._enc_2(h))\n",
    "    h = activation(self._enc_3(h))\n",
    "    return self._residual_stack(h)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "  def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
    "    super(Decoder, self).__init__()\n",
    "    self._num_hiddens = num_hiddens\n",
    "    self._num_residual_layers = num_residual_layers\n",
    "    self._num_residual_hiddens = num_residual_hiddens\n",
    "\n",
    "    self._dec_1 = nn.Conv2D(\n",
    "        in_channels=self._num_hiddens,\n",
    "        out_channels=self._num_hiddens,\n",
    "        kernel_size=(3, 3),\n",
    "        stride=(1, 1))\n",
    "    self._residual_stack = ResidualStack(\n",
    "        self._num_hiddens,\n",
    "        self._num_residual_layers,\n",
    "        self._num_residual_hiddens)\n",
    "    self._dec_2 = nn.ConvTranspose2d(\n",
    "        in_channels=self._num_hiddens,\n",
    "        out_channels=self._num_hiddens // 2,\n",
    "        output_shape=None,\n",
    "        kernel_size=(4, 4),\n",
    "        stride=(2, 2))\n",
    "    self._dec_3 = nn.ConvTranspose2d(\n",
    "        in_channels=3,\n",
    "        out_channels=3,\n",
    "        kernel_size=(4, 4),\n",
    "        stride=(2, 2))\n",
    "\n",
    "  def forward(self, x):\n",
    "    h = self._dec_1(x)\n",
    "    h = self._residual_stack(h)\n",
    "    h = F.relu(self._dec_2(h))\n",
    "    x_recon = self._dec_3(h)\n",
    "\n",
    "    return x_recon\n",
    "\n",
    "\n",
    "class VQVAEModel(nn.Module):\n",
    "  def __init__(self, encoder, decoder, vqvae, pre_vq_conv1,\n",
    "               data_variance):\n",
    "    super(VQVAEModel, self).__init__()\n",
    "    self._encoder = encoder\n",
    "    self._decoder = decoder\n",
    "    self._vqvae = vqvae\n",
    "    self._pre_vq_conv1 = pre_vq_conv1\n",
    "    self._data_variance = data_variance\n",
    "\n",
    "  def forward(self, inputs, is_training):\n",
    "    z = self._pre_vq_conv1(self._encoder(inputs))\n",
    "    vq_output = self._vqvae(z, is_training=is_training)\n",
    "    x_recon = self._decoder(vq_output['quantize'])\n",
    "    recon_error = torch.mean((x_recon - inputs) ** 2) / self._data_variance\n",
    "    loss = recon_error + vq_output['loss']\n",
    "    return {\n",
    "        'z': z,\n",
    "        'x_recon': x_recon,\n",
    "        'loss': loss,\n",
    "        'recon_error': recon_error,\n",
    "        'vq_output': vq_output,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8631078",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Set hyper-parameters.\n",
    "batch_size = 32\n",
    "image_size = 32\n",
    "\n",
    "# 100k steps should take < 30 minutes on a modern (>= 2017) GPU.\n",
    "# 10k steps gives reasonable accuracy with VQVAE on Cifar10.\n",
    "num_training_updates = 10000\n",
    "\n",
    "num_hiddens = 128\n",
    "num_residual_hiddens = 32\n",
    "num_residual_layers = 2\n",
    "# These hyper-parameters define the size of the model (number of parameters and layers).\n",
    "# The hyper-parameters in the paper were (For ImageNet):\n",
    "# batch_size = 128\n",
    "# image_size = 128\n",
    "# num_hiddens = 128\n",
    "# num_residual_hiddens = 32\n",
    "# num_residual_layers = 2\n",
    "\n",
    "# This value is not that important, usually 64 works.\n",
    "# This will not change the capacity in the information-bottleneck.\n",
    "embedding_dim = 64\n",
    "\n",
    "# The higher this value, the higher the capacity in the information bottleneck.\n",
    "num_embeddings = 512\n",
    "\n",
    "# commitment_cost should be set appropriately. It's often useful to try a couple\n",
    "# of values. It mostly depends on the scale of the reconstruction cost\n",
    "# (log p(x|z)). So if the reconstruction cost is 100x higher, the\n",
    "# commitment_cost should also be multiplied with the same amount.\n",
    "commitment_cost = 0.25\n",
    "\n",
    "# Use EMA updates for the codebook (instead of the Adam optimizer).\n",
    "# This typically converges faster, and makes the model less dependent on choice\n",
    "# of the optimizer. In the VQ-VAE paper EMA updates were not used (but was\n",
    "# developed afterwards). See Appendix of the paper for more details.\n",
    "vq_use_ema = True\n",
    "\n",
    "# This is only used for EMA updates.\n",
    "decay = 0.99\n",
    "\n",
    "learning_rate = 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86561784",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Data Loading.\n",
    "train_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(train_data_dict)\n",
    "    .map(cast_and_normalise_images)\n",
    "    .shuffle(10000)\n",
    "    .repeat(-1)  # repeat indefinitely\n",
    "    .batch(batch_size, drop_remainder=True)\n",
    "    .prefetch(-1))\n",
    "\n",
    "valid_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(valid_data_dict)\n",
    "    .map(cast_and_normalise_images)\n",
    "    .repeat(1)  # 1 epoch\n",
    "    .batch(batch_size)\n",
    "    .prefetch(-1))\n",
    "\n",
    "# # Build modules.\n",
    "encoder = Encoder(num_hiddens, num_residual_layers, num_residual_hiddens)\n",
    "decoder = Decoder(num_hiddens, num_residual_layers, num_residual_hiddens)\n",
    "pre_vq_conv1 = snt.Conv2D(output_channels=embedding_dim,\n",
    "    kernel_shape=(1, 1),\n",
    "    stride=(1, 1),\n",
    "    name=\"to_vq\")\n",
    "\n",
    "if vq_use_ema:\n",
    "  vq_vae = snt.nets.VectorQuantizerEMA(\n",
    "      embedding_dim=embedding_dim,\n",
    "      num_embeddings=num_embeddings,\n",
    "      commitment_cost=commitment_cost,\n",
    "      decay=decay)\n",
    "else:\n",
    "  vq_vae = snt.nets.VectorQuantizer(\n",
    "      embedding_dim=embedding_dim,\n",
    "      num_embeddings=num_embeddings,\n",
    "      commitment_cost=commitment_cost)\n",
    "  \n",
    "model = VQVAEModel(encoder, decoder, vq_vae, pre_vq_conv1,\n",
    "                   data_variance=train_data_variance)\n",
    "\n",
    "optimizer = snt.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "@tf.function\n",
    "def train_step(data):\n",
    "  with tf.GradientTape() as tape:\n",
    "    model_output = model(data['image'], is_training=True)\n",
    "  trainable_variables = model.trainable_variables\n",
    "  grads = tape.gradient(model_output['loss'], trainable_variables)\n",
    "  optimizer.apply(grads, trainable_variables)\n",
    "\n",
    "  return model_output\n",
    "\n",
    "train_losses = []\n",
    "train_recon_errors = []\n",
    "train_perplexities = []\n",
    "train_vqvae_loss = []\n",
    "\n",
    "for step_index, data in enumerate(train_dataset):\n",
    "  train_results = train_step(data)\n",
    "  train_losses.append(train_results['loss'])\n",
    "  train_recon_errors.append(train_results['recon_error'])\n",
    "  train_perplexities.append(train_results['vq_output']['perplexity'])\n",
    "  train_vqvae_loss.append(train_results['vq_output']['loss'])\n",
    "\n",
    "  if (step_index + 1) % 100 == 0:\n",
    "    print('%d train loss: %f ' % (step_index + 1,\n",
    "                                   np.mean(train_losses[-100:])) +\n",
    "          ('recon_error: %.3f ' % np.mean(train_recon_errors[-100:])) +\n",
    "          ('perplexity: %.3f ' % np.mean(train_perplexities[-100:])) +\n",
    "          ('vqvae loss: %.3f' % np.mean(train_vqvae_loss[-100:])))\n",
    "  if step_index == num_training_updates:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b29110",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args = Bundle(dict(\n",
    "    seed=123, NSamples=5000,  gpu=0, j=2, gamma=0.7,\n",
    "        batch_size=1024, test_batch_size=1000,  lr=1e-7,\n",
    "    weight_decay=1e-4,  momentum=.9, trainWith='sup',\n",
    "    num_epochs=50, best_loss=float(\"inf\"), NRounds=200,\n",
    "    loop_len=100, resume=False, use_lbfgs=False,\n",
    "    log_interval=10, use_cuda=torch.cuda.is_available(),\n",
    "    ))\n",
    "dev = torch.device(\"cuda\" if args.use_cuda else \"cpu\")\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.set_default_tensor_type(torch.cuda.DoubleTensor)\n",
    "# else:\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "train_kwargs = {'batch_size': args.batch_size,\n",
    "                    'shuffle': True, 'num_workers': args.j}\n",
    "test_kwargs = {'batch_size': args.test_batch_size,\n",
    "                    'shuffle': True, 'num_workers':  args.j}\n",
    "\n",
    "data_kwargs = {'batch_size': args.batch_size,\n",
    "                    'shuffle': True, 'num_workers': 2}\n",
    "if args.use_cuda:\n",
    "    cuda_kwargs = {'num_workers': args.j,\n",
    "                    'pin_memory': True,\n",
    "                    'shuffle': True}\n",
    "    data_kwargs.update(cuda_kwargs)\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    test_kwargs.update(cuda_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1174e178",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
