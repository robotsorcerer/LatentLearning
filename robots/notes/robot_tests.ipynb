{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbdfe9b5-c03b-4373-b3e6-0e4bd75f07f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import torch \n",
    "import random\n",
    "import pygame\n",
    "import numpy as np\n",
    "import torch.nn as nn \n",
    "import numpy.random as npr\n",
    "from datetime import datetime\n",
    "import torch.nn.functional as F\n",
    "from os.path import join, expanduser\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from utility import DataLogger\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da86ff82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "condition_00\n",
      "condition_00/sample_00\n",
      "condition_00/sample_01\n",
      "condition_00/sample_02\n",
      "condition_00/sample_03\n",
      "condition_00/sample_04\n",
      "condition_00/sample_05\n",
      "condition_00/sample_06\n",
      "condition_00/sample_07\n",
      "condition_00/sample_08\n",
      "condition_00/sample_09\n",
      "condition_01\n",
      "condition_01/sample_00\n",
      "condition_01/sample_01\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Unable to open object (object 'OBSERVATIONS_000' doesn't exist)\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2e713906cc58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# collect saved observations and states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrajectory_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mobservation_state_pair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatalogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrajectory_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mobservations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobservation_state_pair\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobservation_state_pair\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MSRProjs/LatentLearner/robots/utility/data_logger.py\u001b[0m in \u001b[0;36mget_state\u001b[0;34m(self, fname, T, dO, dX, dU, verbose)\u001b[0m\n\u001b[1;32m     95\u001b[0m                     \u001b[0;31m#         print(keys, sample_keys, var_keys, df[f\"{keys}/{sample_keys}\"])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# length of episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                         \u001b[0mobs\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"{keys}/{sample_keys}/OBSERVATIONS_{i:0>3}\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m                         \u001b[0meept\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"{keys}/{sample_keys}/END_EFFECTOR_POINTS_{i:0>3}\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                         \u001b[0mjang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"{keys}/{sample_keys}/JOINT_ANGLES_{i:0>3}\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/37/lib/python3.7/site-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    303\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid HDF5 object reference\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m             \u001b[0moid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_e\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m             raise TypeError(\"Accessing a group is done with bytes or str, \"\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5o.pyx\u001b[0m in \u001b[0;36mh5py.h5o.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Unable to open object (object 'OBSERVATIONS_000' doesn't exist)\""
     ]
    }
   ],
   "source": [
    "data_dir = join('..', 'experiments', 'inverted_pendulum', 'data_files')\n",
    "trajectory_samples = sorted(os.listdir(data_dir))\n",
    "trajectory_samples = [join(data_dir, fname) for fname in trajectory_samples]\n",
    "# print(trajectory_samples)\n",
    "datalogger = DataLogger()\n",
    "\n",
    "observation_state_pair = [np.nan for i in range(len(trajectory_samples))]\n",
    "# collect saved observations and states\n",
    "for i in range(len(trajectory_samples)):\n",
    "    observation_state_pair[i] = datalogger.get_state(trajectory_samples[i], verbose=True)\n",
    "observations = np.vstack([obs[0] for obs in observation_state_pair])\n",
    "states = np.vstack([state[1] for state in observation_state_pair])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "074a50a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see: https://github.com/deepmind/sonnet/blob/v2/examples/vqvae_example.ipynb4\n",
    " \n",
    "class ResidualStack(nn.Module):\n",
    "  def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
    "    super(ResidualStack, self).__init__()\n",
    "    self._num_hiddens = num_hiddens\n",
    "    self._num_residual_layers = num_residual_layers\n",
    "    self._num_residual_hiddens = num_residual_hiddens\n",
    "\n",
    "    self._layers = []\n",
    "    for i in range(num_residual_layers):\n",
    "      conv3 = nn.Conv2D(\n",
    "          out_channels=num_residual_hiddens,\n",
    "          kernel_size=(3, 3),\n",
    "          stride=(1, 1))\n",
    "      conv1 = nn.Conv2D(\n",
    "          out_channels=num_hiddens,\n",
    "          kernel_size=(1, 1),\n",
    "          stride=(1, 1))\n",
    "      self._layers.append((conv3, conv1))\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    h = inputs\n",
    "    activation = nn.ReLU()\n",
    "    for conv3, conv1 in self._layers:\n",
    "      conv3_out = conv3(activation(h))\n",
    "      conv1_out = conv1(activation(conv3_out))\n",
    "      h += conv1_out\n",
    "    return activation(h)  # Resnet V1 style\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "  def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
    "    super(Encoder, self).__init__()\n",
    "    self._num_hiddens = num_hiddens\n",
    "    self._num_residual_layers = num_residual_layers\n",
    "    self._num_residual_hiddens = num_residual_hiddens\n",
    "\n",
    "    self._enc_1 = nn.Conv2D(\n",
    "        in_channels=self._num_hiddens // 2, \n",
    "        out_channels=self._num_hiddens // 2,\n",
    "        kernel_size=(4, 4),\n",
    "        stride=(2, 2))\n",
    "    self._enc_2 = nn.Conv2D(\n",
    "        in_channels=self._num_hiddens,\n",
    "        out_channels=self._num_hiddens,\n",
    "        kernel_size=(4, 4),\n",
    "        stride=(2, 2))\n",
    "    self._enc_3 = nn.Conv2D(\n",
    "        in_channels=self._num_hiddens,\n",
    "        out_channels=self._num_hiddens,\n",
    "        kernel_size=(3, 3),\n",
    "        stride=(1, 1),)\n",
    "    self._residual_stack = ResidualStack(\n",
    "        self._num_hiddens,\n",
    "        self._num_residual_layers,\n",
    "        self._num_residual_hiddens)\n",
    "\n",
    "  def forward(self, x):\n",
    "    activation = nn.ReLU()\n",
    "\n",
    "    h = activation(self._enc_1(x))\n",
    "    h = activation(self._enc_2(h))\n",
    "    h = activation(self._enc_3(h))\n",
    "    return self._residual_stack(h)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "  def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
    "    super(Decoder, self).__init__()\n",
    "    self._num_hiddens = num_hiddens\n",
    "    self._num_residual_layers = num_residual_layers\n",
    "    self._num_residual_hiddens = num_residual_hiddens\n",
    "\n",
    "    self._dec_1 = nn.Conv2D(\n",
    "        in_channels=self._num_hiddens,\n",
    "        out_channels=self._num_hiddens,\n",
    "        kernel_size=(3, 3),\n",
    "        stride=(1, 1))\n",
    "    self._residual_stack = ResidualStack(\n",
    "        self._num_hiddens,\n",
    "        self._num_residual_layers,\n",
    "        self._num_residual_hiddens)\n",
    "    self._dec_2 = nn.ConvTranspose2d(\n",
    "        in_channels=self._num_hiddens,\n",
    "        out_channels=self._num_hiddens // 2,\n",
    "        output_shape=None,\n",
    "        kernel_size=(4, 4),\n",
    "        stride=(2, 2))\n",
    "    self._dec_3 = nn.ConvTranspose2d(\n",
    "        in_channels=3,\n",
    "        out_channels=3,\n",
    "        kernel_size=(4, 4),\n",
    "        stride=(2, 2))\n",
    "\n",
    "  def forward(self, x):\n",
    "    h = self._dec_1(x)\n",
    "    h = self._residual_stack(h)\n",
    "    h = F.relu(self._dec_2(h))\n",
    "    x_recon = self._dec_3(h)\n",
    "\n",
    "    return x_recon\n",
    "\n",
    "\n",
    "class VQVAEModel(nn.Module):\n",
    "  def __init__(self, encoder, decoder, vqvae, pre_vq_conv1,\n",
    "               data_variance):\n",
    "    super(VQVAEModel, self).__init__()\n",
    "    self._encoder = encoder\n",
    "    self._decoder = decoder\n",
    "    self._vqvae = vqvae\n",
    "    self._pre_vq_conv1 = pre_vq_conv1\n",
    "    self._data_variance = data_variance\n",
    "\n",
    "  def forward(self, inputs, is_training):\n",
    "    z = self._pre_vq_conv1(self._encoder(inputs))\n",
    "    vq_output = self._vqvae(z, is_training=is_training)\n",
    "    x_recon = self._decoder(vq_output['quantize'])\n",
    "    recon_error = torch.mean((x_recon - inputs) ** 2) / self._data_variance\n",
    "    loss = recon_error + vq_output['loss']\n",
    "    return {\n",
    "        'z': z,\n",
    "        'x_recon': x_recon,\n",
    "        'loss': loss,\n",
    "        'recon_error': recon_error,\n",
    "        'vq_output': vq_output,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8631078",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Set hyper-parameters.\n",
    "batch_size = 32\n",
    "image_size = 32\n",
    "\n",
    "# 100k steps should take < 30 minutes on a modern (>= 2017) GPU.\n",
    "# 10k steps gives reasonable accuracy with VQVAE on Cifar10.\n",
    "num_training_updates = 10000\n",
    "\n",
    "num_hiddens = 128\n",
    "num_residual_hiddens = 32\n",
    "num_residual_layers = 2\n",
    "# These hyper-parameters define the size of the model (number of parameters and layers).\n",
    "# The hyper-parameters in the paper were (For ImageNet):\n",
    "# batch_size = 128\n",
    "# image_size = 128\n",
    "# num_hiddens = 128\n",
    "# num_residual_hiddens = 32\n",
    "# num_residual_layers = 2\n",
    "\n",
    "# This value is not that important, usually 64 works.\n",
    "# This will not change the capacity in the information-bottleneck.\n",
    "embedding_dim = 64\n",
    "\n",
    "# The higher this value, the higher the capacity in the information bottleneck.\n",
    "num_embeddings = 512\n",
    "\n",
    "# commitment_cost should be set appropriately. It's often useful to try a couple\n",
    "# of values. It mostly depends on the scale of the reconstruction cost\n",
    "# (log p(x|z)). So if the reconstruction cost is 100x higher, the\n",
    "# commitment_cost should also be multiplied with the same amount.\n",
    "commitment_cost = 0.25\n",
    "\n",
    "# Use EMA updates for the codebook (instead of the Adam optimizer).\n",
    "# This typically converges faster, and makes the model less dependent on choice\n",
    "# of the optimizer. In the VQ-VAE paper EMA updates were not used (but was\n",
    "# developed afterwards). See Appendix of the paper for more details.\n",
    "vq_use_ema = True\n",
    "\n",
    "# This is only used for EMA updates.\n",
    "decay = 0.99\n",
    "\n",
    "learning_rate = 3e-4\n",
    "\n",
    "\n",
    "# # Data Loading.\n",
    "train_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(train_data_dict)\n",
    "    .map(cast_and_normalise_images)\n",
    "    .shuffle(10000)\n",
    "    .repeat(-1)  # repeat indefinitely\n",
    "    .batch(batch_size, drop_remainder=True)\n",
    "    .prefetch(-1))\n",
    "\n",
    "valid_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(valid_data_dict)\n",
    "    .map(cast_and_normalise_images)\n",
    "    .repeat(1)  # 1 epoch\n",
    "    .batch(batch_size)\n",
    "    .prefetch(-1))\n",
    "\n",
    "# # Build modules.\n",
    "encoder = Encoder(num_hiddens, num_residual_layers, num_residual_hiddens)\n",
    "decoder = Decoder(num_hiddens, num_residual_layers, num_residual_hiddens)\n",
    "pre_vq_conv1 = snt.Conv2D(output_channels=embedding_dim,\n",
    "    kernel_shape=(1, 1),\n",
    "    stride=(1, 1),\n",
    "    name=\"to_vq\")\n",
    "\n",
    "if vq_use_ema:\n",
    "  vq_vae = snt.nets.VectorQuantizerEMA(\n",
    "      embedding_dim=embedding_dim,\n",
    "      num_embeddings=num_embeddings,\n",
    "      commitment_cost=commitment_cost,\n",
    "      decay=decay)\n",
    "else:\n",
    "  vq_vae = snt.nets.VectorQuantizer(\n",
    "      embedding_dim=embedding_dim,\n",
    "      num_embeddings=num_embeddings,\n",
    "      commitment_cost=commitment_cost)\n",
    "  \n",
    "model = VQVAEModel(encoder, decoder, vq_vae, pre_vq_conv1,\n",
    "                   data_variance=train_data_variance)\n",
    "\n",
    "optimizer = snt.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "@tf.function\n",
    "def train_step(data):\n",
    "  with tf.GradientTape() as tape:\n",
    "    model_output = model(data['image'], is_training=True)\n",
    "  trainable_variables = model.trainable_variables\n",
    "  grads = tape.gradient(model_output['loss'], trainable_variables)\n",
    "  optimizer.apply(grads, trainable_variables)\n",
    "\n",
    "  return model_output\n",
    "\n",
    "train_losses = []\n",
    "train_recon_errors = []\n",
    "train_perplexities = []\n",
    "train_vqvae_loss = []\n",
    "\n",
    "for step_index, data in enumerate(train_dataset):\n",
    "  train_results = train_step(data)\n",
    "  train_losses.append(train_results['loss'])\n",
    "  train_recon_errors.append(train_results['recon_error'])\n",
    "  train_perplexities.append(train_results['vq_output']['perplexity'])\n",
    "  train_vqvae_loss.append(train_results['vq_output']['loss'])\n",
    "\n",
    "  if (step_index + 1) % 100 == 0:\n",
    "    print('%d train loss: %f ' % (step_index + 1,\n",
    "                                   np.mean(train_losses[-100:])) +\n",
    "          ('recon_error: %.3f ' % np.mean(train_recon_errors[-100:])) +\n",
    "          ('perplexity: %.3f ' % np.mean(train_perplexities[-100:])) +\n",
    "          ('vqvae loss: %.3f' % np.mean(train_vqvae_loss[-100:])))\n",
    "  if step_index == num_training_updates:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b29110",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.imshow(obs[0])\n",
    "plt.imshow(obs[1])\n",
    "plt.imshow(obs[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b780e276",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('/tmp/test.hdf5', 'a') as fd:\n",
    "    te = fd.create_group(\"test\")\n",
    "    te.create_dataset('array', data=np.arange(1, 10).reshape(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52be51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('/tmp/test.hdf5', 'r+') as fd:\n",
    "    te = fd[\"test/array\"] #, data=np.arange(1, 10).reshape(3,3))\n",
    "    tenp = np.array(te, dtype=te.dtype)\n",
    "    print(tenp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87bbced",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1174e178",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
